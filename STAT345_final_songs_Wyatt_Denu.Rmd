---
title: "STAT 345 Final Project - Over and Over and Over and Over"
author: "STAT 345"
output: html_document
---

Expected Submission: You will generate a well-written R Markdown report that addresses the following prompts. This R Markdown report should source your analysis code and only display top-level, abstracted code *if/when appropriate*. Choices made during the analysis project should be described and justified in the report.

Advice for getting started:

-   Start a conversation chain with your group members. Email is a great place to start, but other options exist (texting, social media platforms, etc.). Set some reasonable expectations for how and when you plan to respond to conversations. It is likely going to be unacceptable to only respond once per week, for example, but also likely unacceptable to expect a response within the hour. Have an honest conversation with your group members about this issue.
-   Start the project from a "top-down design" perspective. So, determine what the major steps of the process are, and determine what the inputs and outputs are for each of those steps (the output of step 1 will likely be the input for step 2, for example). This step is much like writing an outline for a paper before you start to write the paper itself, only much more valuable in a group setting.
-   Once you have a sense of the big picture (the outline), determine some "due dates" for each step of the process. Work backwards from the final submission date, giving yourselves time to work on each of the parts as needed. Given the top-down design, you should be able to "divide and conquer" -- working on parts of the project that depend on earlier steps.
-   Decide how you plan to share code with each other. Using Git and GitHub is a really good choice here. If not, perhaps some form of shared directory online. In a worst-case scenario, email could also work.
-   Be prepared to give Dr. Baumann (at least) weekly updates on your progress. Some of this can be done via email, but discuss with your group about times you are each available to meet online as well (in an office-hour-like setting). Feel free to request meetings with Dr. Baumann to get help.

**General advice:** Get started early. If you wait to the last minute, it will not go well. For this project, you may find yourself spending a reasonable amount of time *searching* for help.

1.  Your first task is to create a list of top songs, dating back to 1958 (when Billboard introduced it's Hot 100 yearly chart). You may want to start with just the yearly top song, but your work should be general enough to account for multiple songs per year. You may narrow your search to a particular genre if you like. You may use any website that provides this information, though you may try to find one that makes part 2 as simple as possible.

2.  For the top songs in part 1, gather some basic information: artist, title, year, genre (if appropriate), length, and other variables you think might be informative (sales figures, etc.).

3.  Find a lyric hosting service (such as www.azlyrics.com or www.songlyrics.com, though these aren't the only options) that provides full lyrics to songs. Ideally, the URLs for these songs follow a reproducible pattern. Write a function that can automatically capture these song lyrics for your top songs from part 1, and then gather the lyrics. Do your best to keep this function general, but you may need to write code for specific instances.

4.  Create two measures of song repetitiveness. Write a function (or two) to measure song repetitiveness, and apply it to each of the songs from part 1. Suggestions for "repetitiveness" include (but are definitely not limited to): "Do songs repeat the same phrase frequently?" and "Do songs repeat their song title frequently"

5.  Have songs become more repetitive over time? Summarize and visualize your repetitive measures from part 4.

6.  (If possible) Extend your work to more songs! Consider more questions, like "Does genre matter?".

```{r}
ggsave("uniqueness_plot.png", width = 8, height = 5, dpi = 300, bg = "white")
```

```{r}
library(readr) # Used for reading the csv file
library(dplyr) # Used for filtering the different songs
library(stringr) # Used for cleaning the lyrics
library(ggplot2) # Used for the graphic that was made

songs <- read_csv("combined_all_songs_lyrics(in).csv", show_col_types = FALSE) # Reads the csv file and stores it as "songs"

songs_cleaned <- songs %>% # Cleans the lyrics so that there is no unecessary characters
  mutate(
    lyrics = lyrics %>%
      str_replace_all("\\\\n|\\n|\\r", " ") %>%
      str_squish(),
    
    total_words = str_count(lyrics, "\\b\\w+\\b"), # Counts the total amount of words that are in each of the songs
    unique_words = sapply(str_split(tolower(lyrics), "\\W+"), function(words) length(unique(words))), # Finds the amount of unique words in each of the songs
    
    uniqueness = ifelse(total_words > 0, unique_words / total_words, NA) # Calculates the uniqueness score for each of the songs and does that by dividing unique words by total words
  )

p3 <- ggplot(songs_cleaned, aes(x = Year, y = uniqueness)) + # Makes the dot plot that contains all of the data points for year and uniqueness score
  geom_point(color = "black", alpha = 0.5, size = 1.4) + # Settings that I chose for the data points
  geom_smooth(method = "lm", se = FALSE, color = "red") + # Regression line for the data
  labs(
    title = "Lyric Uniqueness Score for Top Three Billboard 100 Songs Per Year", # Title of the graphic
    x = "Year", # Title of the x-axis
    y = "Average Uniqueness Score" # Title of the y-axis
  ) +
  theme_minimal() + # Minimal theme
  theme(plot.title = element_text(hjust = 0.5)) + # Centers the title of the graphic
  coord_cartesian(ylim = c(0,1)) # Makes sure that the graphic only shows y-axis going from 0 to 1 because that is where all of the data lies

ggsave("repetitive_lines_frequency.png", plot = p3, width = 8, height = 5, dpi = 300, bg = "white")
```

